---
title: "p8105_hw5_mk4996"
author: "Miho Kawanami"
date: "2025-11-13"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) 
set.seed(1)
```

# Problem 1
```{r}
has_match_once = function(n) {
  bdays = sample(1:365, size = n, replace = TRUE)
  any(duplicated(bdays))
}

sim_p1 = tibble(group_size = 2:50) |>
  mutate(
    trials = map(group_size, \(g) replicate(10000, has_match_once(g))),
    prob   = map_dbl(trials, \(x) mean(x))
  )

sim_p1 |>
  ggplot(aes(x = group_size, y = prob)) +
  geom_line() +
  geom_point() +
  labs(x = "Group size", y = "P(at least one shared birthday)",
       title = "Birthday paradox simulation (10,000 trials each)")
```

# Problem 2

## One trial
```{r}
n     = 30
sigma = 5
alpha = 0.05
mu_grid = c(0,1,2,3,4,5,6)

one_trial = function(mu) {
  x = rnorm(n, mean = mu, sd = sigma)
  tt = t.test(x, mu = 0)
  tibble(
    mu_hat   = unname(tt$estimate),  
    p_value  = tt$p.value
  )
}
```

## Run 5000 simulations for each mu
```{r}
sim_p2 = expand_grid(mu_true = mu_grid, iter = 1:5000) |>
  mutate(res = map(mu_true, one_trial)) |>
  unnest(res)
```

## Compute power and visualize
```{r}
power_df = sim_p2 |>
  group_by(mu_true) |>
  summarize(power = mean(p_value < alpha), .groups = "drop")

power_df |>
  ggplot(aes(x = mu_true, y = power)) +
  geom_line() + geom_point() +
  labs(x = "True mean (mu)", y = "Power",
       title = "One-sample t-test power vs effect size")
```

* **Comment**
The plot shows how power increases as the true mean (Âµ) moves away from zero. When Âµ=0, power is approximately 0.05, matching the significance level (alpha=0.05).
As the true mean increases, the probability of correctly rejecting the null hypothesis rises sharply, reaching nearly 1 for larger effect sizes. This pattern illustrates that stronger effects are easier to detect with a fixed sample size.

## Compare average mu_hat (overall vs significant)
```{r}
avg_df = sim_p2 |>
  group_by(mu_true) |>
  summarize(
    avg_mu_hat_all   = mean(mu_hat),
    avg_mu_hat_reject = mean(mu_hat[p_value < alpha]),
    .groups = "drop"
  )

avg_df |>
  pivot_longer(-mu_true, names_to = "which", values_to = "avg_mu_hat") |>
  ggplot(aes(x = mu_true, y = avg_mu_hat, color = which)) +
  geom_line() + geom_point() +
  labs(x = "True mean (mu)", y = "Average of mu_hat",
       title = "Selection-induced bias when conditioning on rejection")
```

* **Is the sample average of ðœ‡Ì‚ across tests for which the null is rejected approximately equal to the true value of ðœ‡? Why or why not?**

No, the sample average of ðœ‡Ì‚ among rejected tests is not equal to the true value of ðœ‡.
As shown in the second plot, the blue line (significant results) is higher than the red line for small and moderate true means, indicating an upward bias.
This occurs because only extreme sample means tend to be significant.
As the true mean increases and power approaches 1, this bias becomes smaller.

# Problem 3
## Raw Data
```{r}
homi = read_csv("./homicide-data.csv")
names(homi)

homi |> count(disposition, sort = TRUE)
```
There's no  "NA" in disposition.

```{r}
homi |>
  distinct(city, state) |>
  print(n = Inf)
```
There're two errors in cities and states. Modify them below:

## Cleaned the data
```{r}
homi_clean = homi |>
  mutate(state = toupper(state),
         city_state = str_c(city, ", ", state)) |>   
  filter(city_state != "Tulsa, AL")
```

## Calculation, making a table

```{r}
unsolved_levels = c("Closed without arrest", "Open/No arrest")
city_summary = homi_clean |>
  mutate(
    city_state = paste(city, state, sep = ", "),
    unsolved  = disposition %in% unsolved_levels
  ) |>
  group_by(city_state) |>
  summarise(
    n_total    = n(),
    n_unsolved = sum(unsolved)
  ) |>
  mutate(prop_unsolved = n_unsolved / n_total)

city_summary |>
  arrange(desc(n_total)) |>
  knitr::kable(digits = 3,
               col.names = c("City, State", "Total", "Unsolved", "Prop. Unsolved"))
```

* **Comment**
The raw dataset includes *`r nrow(homi)`* homicide records across
*`r dplyr::n_distinct(homi$city)`* large U.S. cities in
*`r dplyr::n_distinct(homi$state)`* states/DC.  
We created `city_state` (e.g., â€œBaltimore, MDâ€) and summarized within cities to obtain
the total number of homicides (*`n_total`*) and the number of unsolved homicides
(`n_unsolved`), where unsolved is defined as *"Closed without arrest"* or *"Open/No arrest"*.

## Baltimore
```{r}

library(broom)

unsolved_levels = c("Closed without arrest", "Open/No arrest")

bal_counts = homi_clean |>
  filter(city == "Baltimore", state == "MD") |>
  mutate(unsolved = disposition %in% unsolved_levels) |>
  summarise(
    n_total    = n(),
    n_unsolved = sum(unsolved, na.rm = TRUE),
    .groups = "drop"
  )

bal_pt = bal_counts |> 
  with(prop.test(n_unsolved, n_total))

bal_tidy = broom::tidy(bal_pt) |>
  select(estimate, conf.low, conf.high)

bal_tidy

```

## All cities 
```{r}

homi_allcities = homi_clean |>
  mutate(city_state = paste(city, state, sep = ", "))

unsolved_levels = c("Closed without arrest", "Open/No arrest")
city_summary = homi_allcities |>
  mutate(unsolved = disposition %in% unsolved_levels) |>
  group_by(city_state) |>
  summarize(
    n_total    = n(),
    n_unsolved = sum(unsolved, na.rm = TRUE),
    .groups = "drop"
  )

one_prop = function(success, total) {
  prop.test(success, total) |>
    broom::tidy() |>
    dplyr::transmute(
      prop_hat = estimate,
      ci_low   = conf.low,
      ci_high  = conf.high
    )
}

city_est = city_summary |>
  mutate(test = purrr::map2(n_unsolved, n_total, one_prop)) |>
  unnest(test)

city_est |>
  arrange(prop_hat) |>
  mutate(city_state = factor(city_state, levels = city_state),
  prop_hat = prop_hat * 100,
    ci_low   = ci_low * 100,
    ci_high  = ci_high * 100
  ) |>
  ggplot(aes(x = city_state, y = prop_hat)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = 0) +
  coord_flip() +
  scale_y_continuous(limits = c(0, 100),  
                     breaks = seq(0, 100, by = 10),  
                     labels = function(x) paste0(x, "%")) + 
  labs(x = NULL, y = "Proportion unsolved (with 95% CI)",
       title = "Unsolved homicide proportion by city")

```

