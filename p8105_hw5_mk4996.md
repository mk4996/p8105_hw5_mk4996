p8105_hw5_mk4996
================
Miho Kawanami
2025-11-13

# Problem 1

``` r
has_match_once = function(n) {
  bdays = sample(1:365, size = n, replace = TRUE)
  any(duplicated(bdays))
}

sim_p1 = tibble(group_size = 2:50) |>
  mutate(
    trials = map(group_size, \(g) replicate(10000, has_match_once(g))),
    prob   = map_dbl(trials, \(x) mean(x))
  )

sim_p1 |>
  ggplot(aes(x = group_size, y = prob)) +
  geom_line() +
  geom_point() +
  labs(x = "Group size", y = "P(at least one shared birthday)",
       title = "Birthday paradox simulation (10,000 trials each)")
```

![](p8105_hw5_mk4996_files/figure-gfm/unnamed-chunk-1-1.png)<!-- -->

# Problem 2

## One trial

``` r
n     = 30
sigma = 5
alpha = 0.05
mu_grid = c(0,1,2,3,4,5,6)

one_trial = function(mu) {
  x = rnorm(n, mean = mu, sd = sigma)
  tt = t.test(x, mu = 0)
  tibble(
    mu_hat   = unname(tt$estimate),  
    p_value  = tt$p.value
  )
}
```

## Run 5000 simulations for each mu

``` r
sim_p2 = expand_grid(mu_true = mu_grid, iter = 1:5000) |>
  mutate(res = map(mu_true, one_trial)) |>
  unnest(res)
```

## Compute power and visualize

``` r
power_df = sim_p2 |>
  group_by(mu_true) |>
  summarize(power = mean(p_value < alpha), .groups = "drop")

power_df |>
  ggplot(aes(x = mu_true, y = power)) +
  geom_line() + geom_point() +
  labs(x = "True mean (mu)", y = "Power (proportion)",
       title = "One-sample t-test power vs effect size")
```

![](p8105_hw5_mk4996_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

- **Comment** The plot shows how power increases as the true mean (Âµ)
  moves away from zero. When Âµ=0, power is approximately 0.05, matching
  the significance level (alpha=0.05). As the true mean increases, the
  probability of correctly rejecting the null hypothesis rises sharply,
  reaching nearly 1 for larger effect sizes. This pattern illustrates
  that stronger effects are easier to detect with a fixed sample size.

## Compare average mu_hat (overall vs significant)

``` r
avg_df = sim_p2 |>
  group_by(mu_true) |>
  summarize(
    avg_mu_hat_all   = mean(mu_hat),
    avg_mu_hat_reject = mean(mu_hat[p_value < alpha]),
    .groups = "drop"
  )

avg_df |>
  pivot_longer(-mu_true, names_to = "which", values_to = "avg_mu_hat") |>
  ggplot(aes(x = mu_true, y = avg_mu_hat, color = which)) +
  geom_line() + geom_point() +
  labs(x = "True mean (mu)", y = "Average estimate of mu_hat",
       title = "Selection-induced bias when conditioning on rejection")
```

![](p8105_hw5_mk4996_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

- **Is the sample average of ðœ‡Ì‚ across tests for which the null is
  rejected approximately equal to the true value of ðœ‡? Why or why not?**

No, the sample average of ðœ‡Ì‚ among rejected tests is not equal to the
true value of ðœ‡. As shown in the second plot, the blue line (significant
results) is higher than the red line for small and moderate true means,
indicating an upward bias. This occurs because only extreme sample means
tend to be significant. As the true mean increases and power approaches
1, this bias becomes smaller.

# Problem 3

## Raw Data

``` r
homi = read_csv("./homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## â„¹ Use `spec()` to retrieve the full column specification for this data.
    ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
names(homi)
```

    ##  [1] "uid"           "reported_date" "victim_last"   "victim_first" 
    ##  [5] "victim_race"   "victim_age"    "victim_sex"    "city"         
    ##  [9] "state"         "lat"           "lon"           "disposition"

``` r
homi |> count(disposition, sort = TRUE)
```

    ## # A tibble: 3 Ã— 2
    ##   disposition               n
    ##   <chr>                 <int>
    ## 1 Closed by arrest      25674
    ## 2 Open/No arrest        23583
    ## 3 Closed without arrest  2922

Thereâ€™s no â€œNAâ€ in disposition.

``` r
homi |>
  distinct(city, state) |>
  print(n = Inf)
```

    ## # A tibble: 51 Ã— 2
    ##    city           state
    ##    <chr>          <chr>
    ##  1 Albuquerque    NM   
    ##  2 Atlanta        GA   
    ##  3 Baltimore      MD   
    ##  4 Baton Rouge    LA   
    ##  5 Birmingham     AL   
    ##  6 Boston         MA   
    ##  7 Buffalo        NY   
    ##  8 Charlotte      NC   
    ##  9 Chicago        IL   
    ## 10 Cincinnati     OH   
    ## 11 Columbus       OH   
    ## 12 Dallas         TX   
    ## 13 Denver         CO   
    ## 14 Detroit        MI   
    ## 15 Durham         NC   
    ## 16 Fort Worth     TX   
    ## 17 Fresno         CA   
    ## 18 Houston        TX   
    ## 19 Indianapolis   IN   
    ## 20 Jacksonville   FL   
    ## 21 Kansas City    MO   
    ## 22 Las Vegas      NV   
    ## 23 Long Beach     CA   
    ## 24 Los Angeles    CA   
    ## 25 Louisville     KY   
    ## 26 Memphis        TN   
    ## 27 Miami          FL   
    ## 28 Milwaukee      wI   
    ## 29 Minneapolis    MN   
    ## 30 Nashville      TN   
    ## 31 New Orleans    LA   
    ## 32 New York       NY   
    ## 33 Oakland        CA   
    ## 34 Oklahoma City  OK   
    ## 35 Omaha          NE   
    ## 36 Philadelphia   PA   
    ## 37 Phoenix        AZ   
    ## 38 Pittsburgh     PA   
    ## 39 Richmond       VA   
    ## 40 San Antonio    TX   
    ## 41 Sacramento     CA   
    ## 42 Savannah       GA   
    ## 43 San Bernardino CA   
    ## 44 San Diego      CA   
    ## 45 San Francisco  CA   
    ## 46 St. Louis      MO   
    ## 47 Stockton       CA   
    ## 48 Tampa          FL   
    ## 49 Tulsa          OK   
    ## 50 Tulsa          AL   
    ## 51 Washington     DC

Thereâ€™re two errors in cities and states. Modify them below:

## Cleaned the data

``` r
homi_clean = homi |>
  mutate(state = toupper(state),
         city_state = str_c(city, ", ", state)) |>   
  filter(city_state != "Tulsa, AL")
```

## Calculation, making a table

``` r
unsolved_levels = c("Closed without arrest", "Open/No arrest")
city_summary = homi_clean |>
  mutate(
    city_state = paste(city, state, sep = ", "),
    unsolved  = disposition %in% unsolved_levels
  ) |>
  group_by(city_state) |>
  summarise(
    n_total    = n(),
    n_unsolved = sum(unsolved),
    .groups = "drop"
  ) |>
  mutate(prop_unsolved = n_unsolved / n_total)

city_summary1_tbl = city_summary |>
  arrange(desc(n_total)) |>
  select(city_state, n_total) |>
  head(3)

city_summary1 = city_summary1_tbl |>
  knitr::kable(digits = 0,
               col.names = c("City, State", "Total"))
city_summary1
```

| City, State      | Total |
|:-----------------|------:|
| Chicago, IL      |  5535 |
| Philadelphia, PA |  3037 |
| Houston, TX      |  2942 |

``` r
city_summary2_tbl = city_summary |>
  arrange(desc(prop_unsolved)) |>
  select(city_state, n_unsolved, prop_unsolved) |>
  head(3)

city_summary2 = city_summary2_tbl  |>
  knitr::kable(digits = 3,
               col.names = c("City, State", "Unsolved", "Prop. Unsolved"))
city_summary2
```

| City, State     | Unsolved | Prop. Unsolved |
|:----------------|---------:|---------------:|
| Chicago, IL     |     4073 |          0.736 |
| New Orleans, LA |      930 |          0.649 |
| Baltimore, MD   |     1825 |          0.646 |

``` r
library(dplyr)

prop_pct = function(df, city) {
  x = df |>
    filter(city_state == city) |>
    pull(prop_unsolved) |>
    first() 
  round(100 * x, 1) 
}

total_n = function(df, city) {
  df |>
    filter(city_state == city) |>
    pull(n_total) |>
    first()
}

chi_total = total_n(city_summary, "Chicago, IL")
chi_prop  = prop_pct(city_summary, "Chicago, IL")
bal_prop  = prop_pct(city_summary, "Baltimore, MD")
nola_prop = prop_pct(city_summary, "New Orleans, LA")
```

- **Comment** The raw dataset includes *52179* homicide records across
  *50* large U.S. cities in *28* states/DC.  
  I created `city_state` (e.g., â€œBaltimore, MDâ€) and summarized within
  cities to obtain the total number of homicides (*`n_total`*) and the
  number of unsolved homicides (`n_unsolved`), where unsolved is defined
  as *â€œClosed without arrestâ€* or *â€œOpen/No arrestâ€*. Chicago, IL had
  the highest total number of homicides (5,535) and one of the highest
  proportions of unsolved cases (73.6%), followed by New Orleans, LA
  (64.9%) and Baltimore, MD (64.6%).

## Baltimore

``` r
library(broom)

unsolved_levels = c("Closed without arrest", "Open/No arrest")

bal_counts = homi_clean |>
  filter(city == "Baltimore", state == "MD") |>
  mutate(unsolved = disposition %in% unsolved_levels) |>
  summarise(
    n_total    = n(),
    n_unsolved = sum(unsolved, na.rm = TRUE),
    .groups = "drop"
  )

bal_pt = bal_counts |> 
  with(prop.test(n_unsolved, n_total))

bal_tidy = broom::tidy(bal_pt) |>
  select(estimate, conf.low, conf.high)

bal_tidy
```

    ## # A tibble: 1 Ã— 3
    ##   estimate conf.low conf.high
    ##      <dbl>    <dbl>     <dbl>
    ## 1    0.646    0.628     0.663

## All cities

``` r
homi_allcities = homi_clean |>
  mutate(city_state = paste(city, state, sep = ", "))

unsolved_levels = c("Closed without arrest", "Open/No arrest")
city_summary = homi_allcities |>
  mutate(unsolved = disposition %in% unsolved_levels) |>
  group_by(city_state) |>
  summarize(
    n_total    = n(),
    n_unsolved = sum(unsolved, na.rm = TRUE),
    .groups = "drop"
  )

one_prop = function(success, total) {
  prop.test(success, total) |>
    broom::tidy() |>
    dplyr::transmute(
      prop_hat = estimate,
      ci_low   = conf.low,
      ci_high  = conf.high
    )
}

city_est = city_summary |>
  mutate(test = purrr::map2(n_unsolved, n_total, one_prop)) |>
  unnest(test)

city_est |>
  arrange(prop_hat) |>
  mutate(city_state = factor(city_state, levels = city_state),
  prop_hat = prop_hat ,
    ci_low   = ci_low ,
    ci_high  = ci_high
  ) |>
  ggplot(aes(x = city_state, y = prop_hat)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = 0) +
  coord_flip() +
  scale_y_continuous(limits = c(0, 1) 
                     ) + 
  labs(x = NULL, y = "Proportion unsolved (with 95% CI)",
       title = "Unsolved homicide proportion by city")
```

![](p8105_hw5_mk4996_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->
